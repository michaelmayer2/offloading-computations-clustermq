---
title: "Offloading Computations to a HPC cluster via clustermq"
author: "Michael Mayer"
format: pdf
editor: visual
---

## ClusterMQ

### Pre-requisite

-   RSW node configured to be a submit node to the HPC cluster

-   RSW and HPC must share

    -   R installation (version and location)

    -   User IDs

    -   location of code (e.g. home-directory)

    -   OS dependency `zeromq` must be installed

## Example

Let's restore the `renv` first:

```{r}
install.packages("renv")
renv::activate()
renv::restore(prompt=FALSE)
```

Set options for clustermq

```{r}
options(clustermq.scheduler = "slurm",
        clustermq.template = "./slurm.tmpl"
)
```

### Load R packages

```{r}
library(clustermq)
library(palmerpenguins)
```

### Write a first compute function

```{r}
# data
x <- as.data.frame(penguins[c(4, 1)])

compute <- function(i,x) {

    ind <- sample(344, 344, replace = TRUE)
    result1 <-
      glm(x[ind, 2] ~ x[ind, 1], family = binomial(logit))
    coefficients(result1)
}

system.time(res<-compute(1,x))
```

### Write a loop

```{r}
library(foreach)
computeloop <- function(trials, x) {
  foreach(i = 1:trials, .combine = rbind) %do% {
    compute(i, x)
  }
}
```

Run 100 & 1000 trials

```{r}
system.time(res <- computeloop(100, x))
system.time(res <- computeloop(1000, x))
```

We are scaling linear to the number of trials.

Could we do any better via any of the apply functions instead of a \`foreach\` loop ?

```{r}
system.time(lapply(1:1000,compute,x=x))
```

This is only marginally faster than the `foreach` loop.

Let's make this faster and go straight to 100 000.

```{r}
trials <- 100000
n_jobs <- 10
chunk_size <- trials/n_jobs/2
system.time(res <- Q(compute, i=1:trials ,const=list(x=x), 
                     n_jobs=n_jobs, log_worker = FALSE, chunk_size=chunk_size))
```

### Alternative approach

One also can register a parallel backend up-front

```{r}
# Register parallel backend to foreach
register_dopar_cmq(
  n_jobs = n_jobs,
  log_worker = FALSE,
  chunk_size = chunk_size
)

```

and then define a parallel compute loop using the `%dopar%`\`directive of `foreach`\`.

```{r}
computepar <- function(trials, n_jobs,x) {
  foreach(
    i = 1:trials, .combine = rbind
  ) %dopar% {
    ind <- sample(344, 344, replace = TRUE)
    result1 <-
      glm(x[ind, 2] ~ x[ind, 1], family = binomial(logit))
    coefficients(result1)
  }
}
```

and run this function

```{r}
system.time(res<-computepar(trials, n_jobs,x))
```

## Summary

We showed a way how to offload computations to a compute backend via `clustermq`.

With the example chosen, we were able to reduce the anticipated compute time of a for loop from 270 to 27 seconds whilst using 10 cores instead of 1. This is almost ideal speed-up and shows the efficiency of `clustermq` when it comes to communication.

If one were to run the same for only 10 000 trials, the speed-up would be only 5x as then the overhead (job submission, collecting results, ...) become more dominant.
